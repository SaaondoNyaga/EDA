# -*- coding: utf-8 -*-
"""
Created on Wed Feb 22 09:34:24 2023

@author: PC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder


######### SECTION 1 #############

##importing my data set (.csv)

df = pd.read_csv('C:\insurance_dataset_capstone.csv')
print(df)

##Checking the data types for each of the columns in the DataFrame
print(df.dtypes)

#Changing some of the data type to a more appropriate data type suning the 
#.astype() method

df['sex']= df['sex'].astype('category')
df['smoker']= df['smoker'].astype('category')
df['region']= df['region'].astype('category')
print(df.dtypes) ##checking the newly updated data types
print(df.columns)

##Handling Missing(Null)Values(NaN) in the data set
#Check if there are any NaN
print(df.isna())

##If there are any, Next is get a summary of null values in each column
print(df.isna().sum()) 

##Calculating the mean and mode values for every column
print(df.mean(numeric_only=True))

print(df['age'].mode()[0])

##Replacing null values in the data set with appropriate values
df['age']= df['age'].fillna(value=df['age'].mode()[0])
df['bmi']= df['bmi'].fillna(value=df['bmi'].mean())
df['children']=df['children'].fillna(value=df['children'].mode()[0])
df['charges']=df['charges'].fillna(value=df['charges'].mean())


#Filling the other missing non numeric values of the data set
df['sex']=df['sex'].fillna(value='male')
df['smoker']=df['smoker'].fillna(value='yes')
df['region']=df['region'].fillna(value='southeast')


##We go ahead to check if the changes have been effected
print(df.isna().sum()) ##The out put shows there are no more missing values


#### ( Observations/ conclusions)
####After importing the data set, the following are my observations
    #1. The data has 1339 rows and 7 columns
    #2. The data types are 
    #age         float64
    #sex          object
    #bmi         float64
    #children    float64
    #smoker       object
    #region       object
    #charges     float64
    #dtype: object
 #But after analysing the data types, i changed "sex","region" and "smoker"
# columns data types to category data type from "object" type,
 # which is more appropriate for them
 
  #3. I inspected the data for missing values, here is what i found
     # age         6
     # sex         1
     # bmi         1
     # children    1
     # smoker      1
     # region      1
     # charges     1

# As required by the project, i filled the numerical data with the mean, mode 
#and other non numerical values of my choice in the missing values


#Conclusion
##The missing values are quite insignificant to affect the outcome of my model

################### SECTION 2 ##############



#Checking for the number of duplicate rows in the dataframe
print(df.duplicated())
print(df.duplicated().sum())
print(df[df.duplicated()]) #To return the the rows that is duplicated using index


#To delete the duplicated row
dl=df.drop([581], axis=0, inplace=True)
print(df.duplicated().sum())
print(df[df.duplicated()])
print(df) #The total number of row is now 1338



##Using  boxplot to check for outliers
sns.boxplot(y='charges', data=df)
plt.show()

sns.boxplot(y='age', data=df)
plt.show()

sns.boxplot(y='bmi', data=df)
plt.show()

sns.boxplot(y='children', data=df)
plt.show()


#Statistical methods for detecting outliers 

##Handling outliers for the 'charges' column
q1=df['charges'].quantile(0.25)
print(q1)

q3=df['charges'].quantile(0.75)
print(q3)


iqr= q3- q1 # (interquartile range)

lower_bound = q1-1.5*(iqr)
upper_bound = q3+1.5*(iqr)


## Handling outliers for the bmi column
Q1=df['bmi'].quantile(0.25)
print(Q1)

Q3=df['bmi'].quantile(0.75)
print(Q3)

print(df['bmi'].describe())
print(df.describe())

IQR= Q3- Q1 # (interquartile range for bmi column)
Lower_bound = Q1-1.5*(IQR)
Upper_bound = Q3+1.5*(IQR)

print("Lower limit for bmi",Lower_bound, "Upper limit for bmi", Upper_bound)

index_list1= df.index[(df['bmi']< Lower_bound) | (df['bmi'] > Upper_bound)]
print("Indexes for outliers in bmi",index_list1)

df = df.drop(index_list1) ##Dropping instances(Rows) that are outliers in bmi
print(df.shape)


##Checking for outliers in charges column
index_list= df.index[(df['charges']< lower_bound) | 
                     (df['charges'] > upper_bound)]
print(index_list)


##Dropping instances(Rows) that are outliers in charges column
df = df.drop(index_list)
print(df.shape)
print(df)  #The number of rows has reduced



##Creating Countplots for categorical data
sns.countplot(x= 'sex', data=df)
plt.show()


sns.countplot(x= 'smoker', data=df)
plt.show()


sns.countplot(x= 'region', data=df)
plt.show()


##Creating a HISTOGRAM for numerical data
sns.displot(df['age'])
plt.show()

sns.displot(df['bmi'], bins=15, kde=True, color='green')
plt.show()


sns.displot(df['charges'], bins=15, kde=True)
plt.show()

sns.displot(df['children'])
plt.show()

### Observations ########

#1. aAfter checking for duplicates, there is a single row that was duplicated 

#2. I used the boxplot to detect outliers: "Charges" and "bmi" have outliers

#3. Using the interquartile range method to calculate bounds and eliminate outliers
## The rows were then reduced to 1193 rows from 1338

#4. Histogram for numeric variables shows that
    
    #a.  The data for charges is highly skewed
    #b. The data for bmi is symmetric
    #c. The data for  age is somewhat skewed
  
##############SECTION 3  #############################################

# Calculating the skew values for the numeric variables
sk=df.skew()
print(sk)


# Checking correlation of the numeric variables with Heatmap

mymatrix= df.corr() # corellation of numeric data set
print(mymatrix)

#Heatmap of the corellation of numeric data
hm=sns.heatmap(mymatrix, annot=True)
print(hm)


##Drawing a Scatter plot with hue parameter

sns.relplot(data=df, x= "children", y="charges",row='smoker',
            hue='smoker', style='smoker', size='age')
plt.show()

sns.relplot(data=df, x= "age", y="charges",col='children', 
            hue='smoker', style='smoker', size='age')
plt.show()


sns.relplot(data=df, x= "age", y="charges",col='region', 
            hue='smoker', style='smoker', size='age')
plt.show()


####### Applying the Min-Max normalization method in scaling my features

y1 = df['charges']
print(type(y1))
print(y1.max())
print(y1.min())
print(y1)

new_y1=(y1-min(y1))/(max(y1)-min(y1))
print(new_y1)
print(new_y1.min())

df['charges']=new_y1
print(df['charges'])

##Scaling the 'bmi' data in the DataFrame

y2 = df['bmi']
print(type(y2))
print(y2.max())
print(y2.min())
print(y2)

new_y2=(y2-min(y2))/(max(y2)-min(y2))
print(new_y2)
df['bmi']=new_y2
print(df['bmi'])

#df=df.drop(['bmi'], axis=1)
y3= df['age']
print(type(y3))
new_y3=(y3-min(y3))/(max(y3)-min(y3))
df['age']=new_y3
print(new_y3)
############## OBSERVATIONS ###############
#(a)
# 1. The age is not skewed from the skew value of 0.075
# 2. The bmi is symmetrical
# 3. The number of children is skewed, though it is not continuous
# 4. The charges is skewed (negative skew)

#(b)
# From the heatmap, i observed that there is No significant correlation
# between the numerical variables

#(c)
# Using the scatter plot with hue parameter, i observed that;

#1. people with 4 to 5 children rarely smoke
#2. people with 0 to 3 children have the highest number of smokers
#3. generally, smokers are charged higher than non smokers
#4. Those who are smokers with children are charged higher at any age
#5. The older people who are non smokers are charged higher regardless of the 
# number of children they have than the younger ones who are non smokers
#6. There is no significant difference in the distribution of smokers across regions

# Conclusion: Those who smoke are relatively charged higher and those with 
# more children find it inappropriate to smoke.

######### SECTION 4 ###################
#Data encoding

from numpy import argmax
from numpy import array

print(type(df['region']))
d= array(df['region'])
print(d)
print(type(d))

print(df['region'].value_counts())


#Using LabelEncoding to convert category data to numerical data

label_enc= LabelEncoder() #(creating an instance)
label_enc= label_enc.fit(df['region'])
print(label_enc.classes_)
print(type(label_enc))
print(label_enc)
 
 
labels2= label_enc.transform(df['region'])
print("For", df['region'], "labels are: ", labels2)
df['region']=labels2


integer_enc= label_enc.fit_transform(df['sex'])
print("\n", integer_enc, df['sex'])

df['sex']=integer_enc

integer_enc2= label_enc.fit_transform(df['smoker'])
print(integer_enc2)
df['smoker']=integer_enc2
print(df['smoker'])
print(df)
print(df.dtypes) #checking the data type of the columns after encoding

##### Observations/ Conclusion

# 1. i used the label encoding technique to convert the  categorical 
# the data types have changed to int64 types
# data to numeric data. the following were assigned automatically by python
    # a. NorthEast    0
    # b. NorthWest    1
    # c. SouthEast    2
    # d. SouthWest    3
    

    #. Sex
      #Female  0
      #Male    1
      
    #Smoker
     # no     0
     # yes    1
     
############### SECTION 5 ####################

##Feature selection


#y(charges) is dependent variable
y= df['charges']
print(y.shape)

#Load x variables into PANDAS DataFrame with columns
x= df.drop(['charges'], axis = 1)
print(x.shape) #(one column has been dropped)
print(y.value_counts())
print(x.columns)



#Splitting the data into train and test data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.3, 
                                                    random_state=10)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
print(x.head(10))
print(x_train.head(10))
print(x_test.head(10))
print(y_test)
print(y_train)

###### Feature Selection using the SelectKBest method

from sklearn.feature_selection import f_regression
from sklearn.feature_selection import SelectKBest



X_new=SelectKBest(f_regression, k=4).fit_transform(x_train,y_train,)

print(X_new.shape)
print(X_new)
print(x_train.columns) #(using the codes to manually identify the columns)
print(x_train.head(10))
print(X_new[0:10])


## Observations

# After performing the splitting of dataset into train and test data using the
# test_size=0.3, using the sklearn library, here are my observations

#1. The x_train dataset has dimension of (835,6), while the x_test has (358,6)

#. After using the k=4 selection to return the best 4 variables that would 
# be required for my model, it returned the following 4 columns:
    # 'age', 'children', 'smoker', 'region'

#I manually identified the columns since X_new returned a numpy array

